{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三方库引入\n",
    "\n",
    "import torch, os\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.engine import build_dataset, build_optimizer, build_scheduler, build_criterion, build_model\n",
    "from utils.util import AvgrageMeter, pearson_correlation_coefficient, update_avg_meters, cal_psd_hr\n",
    "from archs.PAMNet import PAMNet\n",
    "\n",
    "\n",
    "def set_seed(seed=92):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparser引入\n",
    "parser = argparse.ArgumentParser()\n",
    "## ! general params.\n",
    "parser.add_argument('--num_rppg', type=int, default=160, help='the number of rPPG')\n",
    "parser.add_argument('--dataset', type=str, default='VIPL', help='dataset = [VIPL, UBFC, PURE, COHFACE]')\n",
    "parser.add_argument('--dataset_dir', type=str, default='/data2/chushuyang/VIPL', help='dataset dir')\n",
    "parser.add_argument('--vipl_fold', type=int, default=1, help='the fold of VIPL dataset')\n",
    "parser.add_argument('--save_path', type=str, default='', help='the path to save the model [ckpt, code, visulization]')\n",
    "parser.add_argument('--save_mode', type=str, default='all', help='save mode [all, best]')\n",
    "\n",
    "## ! train params.\n",
    "parser.add_argument('--gpu', type=str, default=\"0\", help='gpu id list')\n",
    "parser.add_argument('--img_size', type=int, default=128, help='the length of clip')\n",
    "parser.add_argument('--batch_size', type=int, default=4, help='batch size per gpu')\n",
    "parser.add_argument('--eval_step', type=int, default=1, help='the number of **epochs** to eval')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='the number of epochs to train')\n",
    "parser.add_argument('--echo_batches', type=int, default=500, help='the number of **mini-batches** to print the loss')\n",
    "### loss\n",
    "parser.add_argument('--loss', type=str, default='[\"np_loss\", \"ce_loss\"]', help='loss = [np_loss, ce_loss]')\n",
    "parser.add_argument('--loss_weight', type=str, default='[0.1, 2]', help='loss_weight = [1, 1]')\n",
    "### eval_option\n",
    "parser.add_argument('--eval_type', type=str, default='video', help='eval_type = [clip, video]')\n",
    "\n",
    "## ! model params.\n",
    "parser.add_argument('--model', type=str, default='PAMNet', help='model = [Efficientphys, Physnet, Physformer, PAMNet]')\n",
    "parser.add_argument('--dropout', type=float, default=0.2, help='dropout rate')\n",
    "### optim\n",
    "parser.add_argument('--optim', type=str, default='adam', help='optimizer = [adam, sgd]')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.9, help='beta1 for adam')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for adam')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-5, help='weight decay for optimizer')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum for sgd')\n",
    "### scheduler\n",
    "parser.add_argument('--scheduler', type=str, default='step', help='scheduler = [step]')\n",
    "parser.add_argument('--step_size', type=int, default=50, help='learning rate decay step size')\n",
    "parser.add_argument('--gamma', type=float, default=0.5, help='learning rate decay')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "set_seed(92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型导入\n",
    "rppg_estimator = build_model(args).to(f'cuda:{args.gpu}')\n",
    "ckpt_dir = './ckpts/VIPL_fold_1' \n",
    "epoch_load = 20\n",
    "rppg_estimator.load_state_dict(torch.load(f'{ckpt_dir}/rppg_estimator_{epoch_load}.pth', map_location=torch.device(f'cuda:{args.gpu}')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1009])\n"
     ]
    }
   ],
   "source": [
    "# 数据集导入\n",
    "dataloader_test = build_dataset(args, 'val_video', 1)\n",
    "print(next(iter(dataloader_test))['ecg'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video-level -- test\n",
    "bpm_range = torch.arange(40, 180, dtype=torch.float).cuda()\n",
    "hr_gt = []\n",
    "hr_pred = []\n",
    "\n",
    "device = f'cuda:{args.gpu}'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # num_clip = 3\n",
    "    for sample_batched in tqdm(dataloader_test):\n",
    "        # get the inputs\n",
    "        inputs, ecg, clip_average_HR = sample_batched['video'].to(device),\\\n",
    "            sample_batched['ecg'].to(device), sample_batched['clip_avg_hr'].to(device)\n",
    "\n",
    "        num_clip = 3\n",
    "        input_len = inputs.shape[2]\n",
    "        input_len = input_len - input_len % (num_clip * 32)\n",
    "        clip_len = input_len // num_clip\n",
    "        inputs = inputs[:, :, :input_len, :, :]\n",
    "        ecg = ecg[:, :input_len]\n",
    "\n",
    "        new_args = deepcopy(args)\n",
    "        new_args.num_rppg = clip_len\n",
    "        val_rppg_estimator = build_model(new_args).to(device)\n",
    "        val_rppg_estimator.load_state_dict(torch.load(f'{ckpt_dir}/rppg_estimator_{epoch_load}.pth'))\n",
    "        val_rppg_estimator.eval()\n",
    "        psd_gt_total = 0\n",
    "        psd_pred_total = 0\n",
    "        for idx in range(num_clip):\n",
    "\n",
    "            inputs_iter = inputs[:, :, idx*clip_len : (idx+1)*clip_len, :, :]\n",
    "            ecg_iter = ecg[:, idx*clip_len : (idx+1)*clip_len]\n",
    "\n",
    "            psd_gt = cal_psd_hr(ecg_iter, 30, return_type='psd')\n",
    "            psd_gt_total += psd_gt.view(-1).max(0)[1].cpu() + 40\n",
    "\n",
    "            ## for rppg_estimator:\n",
    "            all_inputs = {\n",
    "                'input_clip': inputs_iter,\n",
    "                'epoch': 11\n",
    "            }\n",
    "            outputs = val_rppg_estimator(all_inputs)\n",
    "            rPPG = outputs['rPPG']\n",
    "\n",
    "            psd_pred = cal_psd_hr(rPPG[0], 30, return_type='psd')\n",
    "            psd_pred_total += psd_pred.view(-1).max(0)[1].cpu() + 40\n",
    "\n",
    "        hr_pred.append(psd_pred_total / num_clip)\n",
    "        hr_gt.append(psd_gt_total / num_clip)\n",
    "\n",
    "\n",
    "print(f'mae of model: {np.mean(np.abs(np.array(hr_gt) - np.array(hr_pred)))}')\n",
    "print(f'rmse of model: {np.sqrt(np.mean(np.square(np.array(hr_gt) - np.array(hr_pred))))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
